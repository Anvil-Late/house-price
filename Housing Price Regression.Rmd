---
title: "Housing Price Regression"
author: "Anvil"
date: "14/09/2020"
output: 
   html_document:
      code_folding: show
      keep_md : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

# Summary

# Loading packages

Throughout this project, we will use :

- ggplot2
- dplyr
- data.table
- corrplot
- caret
- randomForest

```{r}
library(ggplot2)
library(dplyr)
library(data.table)
library(corrplot)
library(caret)
library(randomForest)

```

# Loading the data

As a personal habit, I automate the downloading and loading processes. The original data can be found on [this Kaggle page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)

```{r}

# This will create a "data" folder in the current directory and store the data in it

if (!file.exists("data")){
   dir.create("data")
}

trainurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/5407/868283/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1600334118&Signature=m8A295cdrmCFr1Qo8tweV0krSrzo%2Flis%2B51piiEOmc%2BKUYKQzUshHC2uG00JWzt03KkMDFiz0FpHO95VLzEQFddzD%2B%2Byi43nEGe0XfEXYP7wAKUrEPGTDY0kRtf3Y7n50JDmhp0WGnxzVK47FH1xbuJFacvWhnpAwO78fhVQH6x0HiNsfz8U2rYtkHefSaBy08Dnztggx%2BWEd0Q9vEA4mtqRuT%2Btd4%2FNra2n0BDdZUZ4BDQGaxdXU6aSDoP9XYMv6vbB4%2FFH13YCxbnUr3JuS85g%2FTxM%2Ba5pkmGq1rnijPXHqwRA9WKK%2BFSVhuZ%2Fv3N65u9vUCsnY2kAox26vnd3HQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv"

trainfile <- "./data/train.csv"

testurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/5407/868283/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1600334123&Signature=RgHIdDwVbnPVzd34JSDORiVnsqzsL%2B7OCr%2BQHKjNprCjj58ipSaQLwDKWo73vDLK9hEcRN6kqYMn3%2FVvnO9zX5HY5H%2FCXz6TUoq9iPkA8qLpwPqIv32O96aHb9of6%2BKX%2FLbw1r%2FQKiZOc%2FmIVe13zB7qGcjZaREDJH9v2dBbAIh4SBrc09vgMEzvJS5XqBcw3HuQHmPDfCsboYMFLTH%2F7uV5PPuVvB%2Fr%2BvsDZsR46OOz1gdTKdS1qYcjfa2w%2FJNYpKJ2uAeSAjpulyAKSSyf3FxJQS0EF%2BqqWRrtWE8X%2FIEAZlRKosWHIdLy%2FENyjX0d3Ltpylkd9neTPebQXjj6Yw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.csv"

testfile <- "./data/test.csv"

if (!file.exists(trainfile)){
   download.file(trainurl, trainfile, method = "curl")
}

if (!file.exists(testfile)){
   download.file(testurl, testfile, method = "curl")
}

test <- read.csv(testfile)
train <- read.csv(trainfile)
```

# Exploratory data analysis

It's now time to take a look at our data. The first thing we'll do is to convert both train and test sets to data tables to make future modifications easier. 

```{r}
train <- data.table(train)
test <- data.table(test)
```

## Data Partition

We'll split our training set into a sub-train and sub-test set with a 70-30 partition :

```{r}
splitter <- createDataPartition(y=train$SalePrice, p=0.7, list = F)
subtest <- train[-splitter, ]
train <- train[splitter, ]
```

## Variables' class

Let's have a look at our training set, and especially the class of its variables :
```{r}
train[, lapply(.SD, class), .SDcols = names(train[,1:14])]
```
We have a lot of character variables; the [data description file provided by kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) indicates that these are classification variables with only a few possible values. They should therefore be converted to factors.

```{r}
to_factor <- names(which(sapply(train, class) == "character"))
```

Similarly, while some of the integer or numeric vectors indicate a truly numeric value, such as *LotArea* which indicates the lot size in square feet, others are - again - classification variables, such as *MSSubClass*, which identifies the type of dwelling involved in the sale, with, for instance, 20 signifying "1-STORY 1946 & NEWER ALL STYLES" (see data description file). 
We have 3 such variables : *MSSubClass*, *OverallQual*, and *OverallCond*.

These will also have to be converted to factor.


```{r}
to_factor <- c(to_factor,"MSSubClass", "OverallQual", "OverallCond")

```



## Dates

We have 4 variables that contain date information :

- YearBuilt: Original construction date
- YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)
- GarageYrBlt: Year garage was built
- YrSold: Year Sold

Intuitively, it can be said that these variables, especially the construction date, can have an impact on the final price, which seems to be confirmed in this plot :

```{r}
qplot(train$YearBuilt, train$SalePrice) + labs(x = "Construction Date",
                                               y = "House Sale Price") +
   ggtitle("House sale price by year of construction")

```

Looking at the range of dates and their linear relationship with the sale price, we'll keep the dates variables as integer.

However, the *GarageYrBlt* variable is problematic since it is set to NA when there is no garage, and can cause trouble when we preprocess missing values (we'll get to NAs in general in a minute). 

Therefore, we'll either remove this variable if it contributes little to the data set or convert it to factor with a "none" level if it does.

We'll check if this variable is highly correlated to others. Let's see the correlation percentage between *YearBuilt* and *GarageYrBlt* :

```{r}
cor(train$YearBuilt, train$GarageYrBlt, use = "complete")
```
Therefore, we can say that this variable contributes little to the dataset, and we can safely remove it.

## Missing values (NAs) 

Before we convert these variables to factor and go forward into the analysis, we need to first have a look at the proportion of NAs in the dataset.

Let's first see which columns have NAs :

```{r}
temp_na <- as.numeric(train[, lapply(.SD, function(y){sum(is.na(y))})])
names(temp_na) <- names(train)
temp_na <- temp_na[temp_na > 0]
print(temp_na)
```

We have about 4900 NAs, which is around 6% of our data. However, the description file indicates that for some variables, NA doesn't mean a missing value but an absence of feature. For instance, for the *Alley* column, NA signifiest that there is no alley access.

Therefore, in some cases, NAs contain valuable information, and since we might do some pre-processing to convert missing values later on and we don't want to change this type of NA, we'll change them to a "None" level for factor variables.

The factor variables concerned by this are *Alley*, *BsmtQual*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2*, *FireplaceQu*, *GarageType*, *GarageFinish*, *GarageQual*, *GarageCond*, *PoolQC*, *Fence*, and *MiscFeature*. As seen previously, we ignore *GarageYrBlt* since we will remove this variable


```{r}
temp_na2 <- temp_na[temp_na> 10][-c(1,10)]

print(temp_na2)
```

## Data cleaning

We will now take all that have been said so far to clean the data. We will also remove the first column since it is the ID variable for the various houses in our dataset.

```{r}
# Step 1 : convert NAs 
for (j in names(temp_na2)){
   set(train,which(is.na(train[[j]])),j,"None")
}

# Step 2 :  convert to factor

train[, (to_factor) := lapply(.SD, as.factor), .SDcols = to_factor]

# step 3 : Delete ID and GarageYrBuilt variables
train <- train[, -c(1,60)]
```

## Correlation

Since there are 81 total variables, of which 33 are numeric, it might be useful to look for redundancies between variables.

In other words, we would like to se a correlation plot of numeric variables and remove those that are heavily correlated to others :

```{r}
integer_cols <- sapply(train, function(y){class(y) == "integer"})
integer_train <- train[, .SD, .SDcols = integer_cols]
integer_train <- integer_train[, 1:32] # remove "SalePrice column

train_cor <- cor(integer_train, use = "complete")
corrplot2(train_cor, method = "square", type = "lower", 
         title = "Correlation plot between numeric variables",
         tl.col = "black", tl.cex = 0.6, order = "hclust",
         mar = c(0,0,2,0))
```
We find that there are some variables that have strong correlations with each other :

- GarageCars and GarageArea
- 1stFlrSF (First Floor square feet) and TotalBsmtSF (Total square feet of basement area)
- TotRmsAbvGrd (Total rooms above grade) and GrLivArea (Above grade living area square feet)

Below are their correlation coefficients :

```{r}
cor(train$GarageCars, train$GarageArea)
cor(train$X1stFlrSF, train$TotalBsmtSF)
cor(train$TotRmsAbvGrd, train$GrLivArea)
```

Moreover, GarageCars, GrLivArea and TotalBsmtSF are heavily correlated to other variables. We will therefore remove them from the dataset.

```{r}
train <- train[, -c(38, 46, 60)]
```

# Preprocessing

Now that we looked at our data, cleaned it and selected the variables that we will use, it is time to do some pre-processing. Here, we will fill in the NAs. Given the low proportion of NAs, the dataset's size and the general similarities between rows and variables, we will use the KNN-Impute algorithm.

```{r}
preObj <- preProcess(train[, -76], method = "knnImpute")
proc_train <- data.table(cbind(predict(preObj, train[, -76]), train[, 76]))
```

