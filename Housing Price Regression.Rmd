---
title: "Housing Price Regression"
author: "Anvil"
date: "14/09/2020"
output: 
   html_document:
      code_folding: show
      keep_md : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

# Summary

# Loading packages

Throughout this project, we will use the following packages:


```{r}
library(ggplot2)
library(ggrepel)
library(dplyr)
library(data.table)
library(corrplot)
library(caret)
library(randomForest)
library(gbm)
library(gridExtra)
library(scales)
```

# Loading the data

As a personal habit, I automate the downloading and loading processes. The original data can be found on [this Kaggle page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)

```{r}

# This will create a "data" folder in the current directory and store the data in it

if (!file.exists("data")){
   dir.create("data")
}

trainurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/5407/868283/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1600334118&Signature=m8A295cdrmCFr1Qo8tweV0krSrzo%2Flis%2B51piiEOmc%2BKUYKQzUshHC2uG00JWzt03KkMDFiz0FpHO95VLzEQFddzD%2B%2Byi43nEGe0XfEXYP7wAKUrEPGTDY0kRtf3Y7n50JDmhp0WGnxzVK47FH1xbuJFacvWhnpAwO78fhVQH6x0HiNsfz8U2rYtkHefSaBy08Dnztggx%2BWEd0Q9vEA4mtqRuT%2Btd4%2FNra2n0BDdZUZ4BDQGaxdXU6aSDoP9XYMv6vbB4%2FFH13YCxbnUr3JuS85g%2FTxM%2Ba5pkmGq1rnijPXHqwRA9WKK%2BFSVhuZ%2Fv3N65u9vUCsnY2kAox26vnd3HQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv"

trainfile <- "./data/train.csv"

testurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/5407/868283/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1600334123&Signature=RgHIdDwVbnPVzd34JSDORiVnsqzsL%2B7OCr%2BQHKjNprCjj58ipSaQLwDKWo73vDLK9hEcRN6kqYMn3%2FVvnO9zX5HY5H%2FCXz6TUoq9iPkA8qLpwPqIv32O96aHb9of6%2BKX%2FLbw1r%2FQKiZOc%2FmIVe13zB7qGcjZaREDJH9v2dBbAIh4SBrc09vgMEzvJS5XqBcw3HuQHmPDfCsboYMFLTH%2F7uV5PPuVvB%2Fr%2BvsDZsR46OOz1gdTKdS1qYcjfa2w%2FJNYpKJ2uAeSAjpulyAKSSyf3FxJQS0EF%2BqqWRrtWE8X%2FIEAZlRKosWHIdLy%2FENyjX0d3Ltpylkd9neTPebQXjj6Yw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.csv"

testfile <- "./data/test.csv"

if (!file.exists(trainfile)){
   download.file(trainurl, trainfile, method = "curl")
}

if (!file.exists(testfile)){
   download.file(testurl, testfile, method = "curl")
}

test <- read.csv(testfile)
train <- read.csv(trainfile)
```

# Exploratory data analysis

It's now time to take a look at our data. The first thing we'll do is to convert both train and test sets to data tables to make future modifications easier. 

```{r}
train <- data.table(train)
test <- data.table(test)
```

## Data Partition

Although Kaggle provides us with a test set, we might want to do some testing and cross validation ourselves before submitting our model to see if we have a good fit.

Therefore, we'll split our training set into a sub-train and sub-test set with a 70-30 partition :

```{r}
set.seed(1234)
splitter <- createDataPartition(y=train$SalePrice, p=0.7, list = F)
subtest <- train[-splitter, ]
train <- train[splitter, ]
```

## Dimensions

Let's have a look at the dimensions of our training set :

```{r}
dim(train)
```

We have a set that has a lot of variables, 81 in total, but is overall quite small, with only 1024 observations.
Let's have a look at the variables we have here :

```{r}
names(train)
```

## Exploring the response variable

Our response variable here that we want to predict is *SalePrice*. The first exploratory data analysis we want here is to have a look at its distribution :

```{r}
ggplot(data = train, aes(x = SalePrice)) + geom_histogram(fill="blue")
```
Here, we can see that *SalePrice* has a skewed distribution. We can highlight that even more with a density plot (in blue) compared to a normal distribution (in red) :

```{r}

ggplot(data = train, aes(x = SalePrice)) + geom_density(fill="blue") + 
   stat_function(fun = dnorm, 
                 args = list(mean = mean(train$SalePrice), sd = sd(train$SalePrice)), 
                 colour = "red")
```

Generally, models will tend to predict normally distributed data. Therefore, our response being skewed means that any model we build will have a higher than normal inaccuracy unless we apply a transformation to make the data gaussian, such as predicting logs or a Box-Cox transformation. We will get to that during the pre-processing phase.


## Variables' class

Let's have a look at the variables' class :
```{r}
train[, lapply(.SD, class), .SDcols = names(train[,1:14])]
```
We have a lot of character variables; the [data description file provided by kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) indicates that these are classification variables with only a few possible values. They should therefore be converted to factors.

```{r}
to_factor <- names(which(sapply(train, class) == "character"))
```

Similarly, while some of the integer or numeric vectors indicate a truly numeric value, such as *LotArea* which indicates the lot size in square feet, others are - again - classification variables, such as *MSSubClass*, which identifies the type of dwelling involved in the sale, with, for instance, 20 signifying "1-STORY 1946 & NEWER ALL STYLES" (see data description file). 
We have 3 such variables : *MSSubClass*, *OverallQual*, and *OverallCond*.

These will also have to be converted to factor. However, for now it suits us to keep them as numeric, so we'll convert them later on


## Dates

We have 4 variables that contain date information :

- YearBuilt: Original construction date
- YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)
- GarageYrBlt: Year garage was built
- YrSold: Year Sold

Intuitively, it can be said that these variables, especially the construction date, can have an impact on the final price, which seems to be confirmed in this plot :

```{r}
qplot(train$YearBuilt, train$SalePrice) + labs(x = "Construction Date",
                                               y = "House Sale Price") +
   ggtitle("House sale price by year of construction")

```

Looking at the range of dates and their linear relationship with the sale price, we'll keep the dates variables as integer. 

However, the *GarageYrBlt* variable is problematic since it is set to NA when there is no garage, and can cause trouble when we preprocess missing values (we'll get to NAs in general in a minute). 

Therefore, we'll either remove this variable if it contributes little to the data set or convert it to factor with a "none" level if it does.

We'll check if this variable is highly correlated to others. Let's see the correlation percentage between *YearBuilt* and *GarageYrBlt* :

```{r}
cor(train$YearBuilt, train$GarageYrBlt, use = "complete")
```
Therefore, we can say that this variable contributes little to the dataset, and we can safely remove it.

## Missing values (NAs) 

Before we convert these variables to factor and go forward into the analysis, we need to first have a look at the proportion of NAs in the dataset.

Let's first see which columns have NAs :

```{r}
temp_na <- as.numeric(train[, lapply(.SD, function(y){sum(is.na(y))})])
names(temp_na) <- names(train)
temp_na <- temp_na[temp_na > 0]
print(temp_na)
```

We have `r sum(temp_na)` NAs, which is around `r round(100*sum(temp_na) / (dim(train)[1] * dim(train)[2]))`% of our data. However, the description file indicates that for some variables, NA doesn't mean a missing value but an absence of feature. For instance, for the *Alley* column, NA signifiest that there is no alley access.

Therefore, in some cases, NAs contain valuable information, and since we might do some pre-processing to convert missing values later on and we don't want to change this type of NA, we'll change them to a "None" level for factor variables.

The factor variables concerned by this are *Alley*, *BsmtQual*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2*, *FireplaceQu*, *GarageType*, *GarageFinish*, *GarageQual*, *GarageCond*, *PoolQC*, *Fence*, and *MiscFeature*. As seen previously, we ignore *GarageYrBlt* since we will remove this variable


```{r}
# Factor variables for which NA means "none" are mostly those with more that 10NAs,
# except LotFrontage. We remove GarageYrBlt, thus the -c(1, 10)
temp_na2 <- temp_na[temp_na> 10][-c(1,10)]

print(temp_na2)
```

## Data cleaning

We will now take all that have been said so far to clean the data. We will also remove the first column since it is the ID variable for the various houses in our dataset.

```{r}
# Step 1 : convert NAs 
for (j in names(temp_na2)){
   set(train,which(is.na(train[[j]])),j,"None")
}

# Step 2 :  convert to factor

train[, (to_factor) := lapply(.SD, as.factor), .SDcols = to_factor]

# step 3 : Delete ID and GarageYrBuilt variables
train <- train[, -c(1,60)]
```

## Correlation

Of our 79 remaining variables, 36 are numeric :

```{r}
sum(sapply(train, function(y){class(y) == "integer"}))
```

Now, we want to see a correlation plot. This will allow us to see which variables are the most correlated to *SalePrice*, and also which ones are highly correlated between each other. Ideally, we want to maximise the number of numeric variables we can use, which explains why we didn't convert some of them to factor yet.


```{r, echo=F}
# This is a modified version of corrplot that prevents labels to go out of frame
corrplot2 <- 
   function (corr, method = c("circle", "square", "ellipse", "number", 
                              "shade", "color", "pie"), type = c("full", "lower", "upper"), 
             add = FALSE, col = NULL, bg = "white", title = "", is.corr = TRUE, 
             diag = TRUE, outline = FALSE, mar = c(0, 0, 0, 0), addgrid.col = NULL, 
             addCoef.col = NULL, addCoefasPercent = FALSE, order = c("original", 
                                                                     "AOE", "FPC", "hclust", "alphabet"), hclust.method = c("complete", 
                                                                                                                            "ward", "ward.D", "ward.D2", "single", "average", "mcquitty", 
                                                                                                                            "median", "centroid"), addrect = NULL, rect.col = "black", 
             rect.lwd = 2, tl.pos = NULL, tl.cex = 1, tl.col = "red", 
             tl.offset = 0.4, tl.srt = 90, cl.pos = NULL, cl.lim = NULL, 
             cl.length = NULL, cl.cex = 0.8, cl.ratio = 0.15, cl.align.text = "c", 
             cl.offset = 0.5, number.cex = 1, number.font = 2, number.digits = NULL, 
             addshade = c("negative", "positive", "all"), shade.lwd = 1, 
             shade.col = "white", p.mat = NULL, sig.level = 0.05, insig = c("pch", 
                                                                            "p-value", "blank", "n"), pch = 4, pch.col = "black", 
             pch.cex = 3, plotCI = c("n", "square", "circle", "rect"), 
             lowCI.mat = NULL, uppCI.mat = NULL, na.label = "?", na.label.col = "black",
             h.ratio = FALSE,
             ...){
      method <- match.arg(method)
      type <- match.arg(type)
      order <- match.arg(order)
      hclust.method <- match.arg(hclust.method)
      plotCI <- match.arg(plotCI)
      insig <- match.arg(insig)
      if (!is.matrix(corr) && !is.data.frame(corr)) {
         stop("Need a matrix or data frame!")
      }
      if (is.null(addgrid.col)) {
         addgrid.col <- switch(method, color = NA, shade = NA, 
                               "grey")
      }
      if (any(corr < cl.lim[1]) || any(corr > cl.lim[2])) {
         stop("color limits should cover matrix")
      }
      if (is.null(cl.lim)) {
         if (is.corr) {
            cl.lim <- c(-1, 1)
         }
         else {
            cl.lim <- c(min(corr), max(corr))
         }
      }
      intercept <- 0
      zoom <- 1
      if (!is.corr) {
         if (max(corr) * min(corr) < 0) {
            intercept <- 0
            zoom <- 1/max(abs(cl.lim))
         }
         if (min(corr) >= 0) {
            intercept <- -cl.lim[1]
            zoom <- 1/(diff(cl.lim))
         }
         if (max(corr) <= 0) {
            intercept <- -cl.lim[2]
            zoom <- 1/(diff(cl.lim))
         }
         corr <- (intercept + corr) * zoom
      }
      cl.lim2 <- (intercept + cl.lim) * zoom
      int <- intercept * zoom
      if (min(corr, na.rm = TRUE) < -1 - .Machine$double.eps^0.75 || 
          max(corr, na.rm = TRUE) > 1 + .Machine$double.eps^0.75) {
         stop("The matrix is not in [-1, 1]!")
      }
      if (is.null(col)) {
         col <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", 
                                   "#F4A582", "#FDDBC7", "#FFFFFF", "#D1E5F0", "#92C5DE", 
                                   "#4393C3", "#2166AC", "#053061"))(200)
      }
      n <- nrow(corr)
      m <- ncol(corr)
      min.nm <- min(n, m)
      ord <- seq_len(min.nm)
      if (order != "original") {
         ord <- corrMatOrder(corr, order = order, hclust.method = hclust.method)
         corr <- corr[ord, ord]
      }
      if (is.null(rownames(corr))) {
         rownames(corr) <- seq_len(n)
      }
      if (is.null(colnames(corr))) {
         colnames(corr) <- seq_len(m)
      }
      apply_mat_filter <- function(mat) {
         x <- matrix(1:n * m, n, m)
         switch(type, upper = mat[row(x) > col(x)] <- Inf, lower = mat[row(x) < 
                                                                          col(x)] <- Inf)
         if (!diag) {
            diag(mat) <- Inf
         }
         return(mat)
      }
      getPos.Dat <- function(mat) {
         tmp <- apply_mat_filter(mat)
         Dat <- tmp[is.finite(tmp)]
         ind <- which(is.finite(tmp), arr.ind = TRUE)
         Pos <- ind
         Pos[, 1] <- ind[, 2]
         Pos[, 2] <- -ind[, 1] + 1 + n
         return(list(Pos, Dat))
      }
      getPos.NAs <- function(mat) {
         tmp <- apply_mat_filter(mat)
         ind <- which(is.na(tmp), arr.ind = TRUE)
         Pos <- ind
         Pos[, 1] <- ind[, 2]
         Pos[, 2] <- -ind[, 1] + 1 + n
         return(Pos)
      }
      Pos <- getPos.Dat(corr)[[1]]
      n2 <- max(Pos[, 2])
      n1 <- min(Pos[, 2])
      nn <- n2 - n1
      newrownames <- as.character(rownames(corr)[(n + 1 - n2):(n + 
                                                                  1 - n1)])
      m2 <- max(Pos[, 1])
      m1 <- min(Pos[, 1])
      mm <- max(1, m2 - m1)
      newcolnames <- as.character(colnames(corr)[m1:m2])
      DAT <- getPos.Dat(corr)[[2]]
      len.DAT <- length(DAT)
      assign.color <- function(dat = DAT, color = col) {
         newcorr <- (dat + 1)/2
         newcorr[newcorr <= 0] <- 0
         newcorr[newcorr >= 1] <- 1 - 1e-16
         color[floor(newcorr * length(color)) + 1]
      }
      col.fill <- assign.color()
      isFALSE <- function(x) identical(x, FALSE)
      isTRUE <- function(x) identical(x, TRUE)
      if (isFALSE(tl.pos)) {
         tl.pos <- "n"
      }
      if (is.null(tl.pos) || isTRUE(tl.pos)) {
         tl.pos <- switch(type, full = "lt", lower = "ld", upper = "td")
      }
      if (isFALSE(cl.pos)) {
         cl.pos <- "n"
      }
      if (is.null(cl.pos) || isTRUE(cl.pos)) {
         cl.pos <- switch(type, full = "r", lower = "b", upper = "r")
      }
      if (isFALSE(outline)) {
         col.border <- col.fill
      }
      if (isTRUE(outline)) {
         col.border <- "black"
      }
      if (is.character(outline)) {
         col.border <- outline
      }
      oldpar <- par(mar = mar, bg = "white")
      on.exit(par(oldpar), add = TRUE)
      if (!add) {
         plot.new()
         xlabwidth <- max(strwidth(newrownames, cex = tl.cex))
         ylabwidth <- max(strwidth(newcolnames, cex = tl.cex))
         laboffset <- strwidth("W", cex = tl.cex) * tl.offset 
         
         for (i in 1:50) { # 200
            
            xlim <- c(
               m1 - 0.5 - xlabwidth * (grepl("l", tl.pos) | grepl("d", tl.pos)) - laboffset
               , 
               m2 + 0.5 + mm * cl.ratio * (cl.pos == "r") +
                  xlabwidth * abs(cos(tl.srt * pi/180)) * grepl("d", tl.pos) 
            ) + c(-0.35, 0.15)
            + c(-1,0) * grepl("l", tl.pos) # margin between text and grid
            
            ylim <- c(
               n1 - 0.5 - nn * cl.ratio * (cl.pos == "b")
               , 
               # n2 + 0.5 + ylabwidth + laboffset +
               n2 + 0.5 + laboffset + 
                  ylabwidth * abs(sin(tl.srt * pi/180)) * (grepl("d", tl.pos) | grepl("t", tl.pos)) 
            ) + c(-0.15, 0.2)
            + c(0,1) * grepl("d", tl.pos) # margin between text and grid
            
            plot.window(xlim, ylim,     
                        # plot.window(xlim + c(-0.35, 0.15), ylim + c(-0.15, 0.35), 
                        asp = 1, xaxs = "i", yaxs = "i")
            x.tmp <- max(strwidth(newrownames, cex = tl.cex))
            y.tmp <- max(strwidth(newcolnames, cex = tl.cex))
            laboffset.tmp <- strwidth("W", cex = tl.cex) * tl.offset            
            # if (min(x.tmp - xlabwidth, y.tmp - ylabwidth) < 1e-03) {
            if (max(x.tmp - xlabwidth, y.tmp - ylabwidth, laboffset.tmp - laboffset) < 1e-03) {
               break
            }
            xlabwidth <- x.tmp
            ylabwidth <- y.tmp
            laboffset <- laboffset.tmp
            
            if (i == 50) {
               warning(c("Not been able to calculate text margin, ",
                         "please try again with a clean new empty window using {plot.new(); dev.off()} ",
                         "or reduce tl.cex"))
            }
         }
         
         #         if (tl.pos == "n" || tl.pos == "d") {
         #             xlabwidth <- ylabwidth <- 0
         #         }
         #         if (tl.pos == "td") {
         #             ylabwidth <- 0
         #         }
         #         if (tl.pos == "ld") 
         # xlabwidth <- 0 # Modified by Seb
         #         laboffset <- strwidth("W", cex = tl.cex) * tl.offset
         #         xlim <- c(m1 - 0.5 - xlabwidth - laboffset, m2 + 0.5 + 
         #             mm * cl.ratio * (cl.pos == "r") + xlabwidth * cos(tl.srt * pi/180)) + c(-0.35, 0.15)
         #             # mm * cl.ratio * (cl.pos == "r")) + c(-0.35, 0.15))
         #         ylim <- c(n1 - 0.5 - nn * cl.ratio * (cl.pos == "b"), 
         #             n2 + 0.5 + ylabwidth * abs(sin(tl.srt * pi/180)) + 
         #                 laboffset) + c(-0.15, 0.35)
         
         
         if (.Platform$OS.type == "windows") {
            grDevices::windows.options(width = 7, height = 7 * 
                                          diff(ylim)/diff(xlim))
         } else {
            if (round(diff(ylim)/diff(xlim), digits = 1) != 1.0) {
               message(paste0("Suggested windows size: height = width * ", round(diff(ylim)/diff(xlim), digits = 2)))
            }
         }
         if (h.ratio) {
            h.ratio.val <- diff(ylim)/diff(xlim)
         }
         
         plot.window(xlim = xlim, ylim = ylim, asp = 1, xlab = "", 
                     ylab = "", xaxs = "i", yaxs = "i")
      }
      laboffset <- strwidth("W", cex = tl.cex) * tl.offset
      symbols(Pos, add = TRUE, inches = FALSE, squares = rep(1, 
                                                             len.DAT), bg = bg, fg = bg)
      if (method == "circle" && plotCI == "n") {
         symbols(Pos, add = TRUE, inches = FALSE, circles = 0.9 * 
                    abs(DAT)^0.5/2, fg = col.border, bg = col.fill)
      }
      if (method == "ellipse" && plotCI == "n") {
         ell.dat <- function(rho, length = 99) {
            k <- seq(0, 2 * pi, length = length)
            x <- cos(k + acos(rho)/2)/2
            y <- cos(k - acos(rho)/2)/2
            return(cbind(rbind(x, y), c(NA, NA)))
         }
         ELL.dat <- lapply(DAT, ell.dat)
         ELL.dat2 <- 0.85 * matrix(unlist(ELL.dat), ncol = 2, 
                                   byrow = TRUE)
         ELL.dat2 <- ELL.dat2 + Pos[rep(1:length(DAT), each = 100), 
         ]
         polygon(ELL.dat2, border = col.border, col = col.fill)
      }
      if (is.null(number.digits)) {
         number.digits <- switch(addCoefasPercent + 1, 2, 0)
      }
      stopifnot(number.digits%%1 == 0)
      stopifnot(number.digits >= 0)
      if (method == "number" && plotCI == "n") {
         text(Pos[, 1], Pos[, 2], font = number.font, col = col.fill, 
              labels = round((DAT - int) * ifelse(addCoefasPercent, 
                                                  100, 1)/zoom, number.digits), cex = number.cex)
      }
      NA_LABEL_MAX_CHARS <- 2
      if (any(is.na(corr)) && is.character(na.label)) {
         PosNA <- getPos.NAs(corr)
         if (na.label == "square") {
            symbols(PosNA, add = TRUE, inches = FALSE, squares = rep(1, 
                                                                     nrow(PosNA)), bg = na.label.col, fg = na.label.col)
         }
         else if (nchar(na.label) %in% 1:NA_LABEL_MAX_CHARS) {
            symbols(PosNA, add = TRUE, inches = FALSE, squares = rep(1, 
                                                                     nrow(PosNA)), fg = bg, bg = bg)
            text(PosNA[, 1], PosNA[, 2], font = number.font, 
                 col = na.label.col, labels = na.label, cex = number.cex, 
                 ...)
         }
         else {
            stop(paste("Maximum number of characters for NA label is:", 
                       NA_LABEL_MAX_CHARS))
         }
      }
      if (method == "pie" && plotCI == "n") {
         symbols(Pos, add = TRUE, inches = FALSE, circles = rep(0.5, 
                                                                len.DAT) * 0.85)
         pie.dat <- function(theta, length = 100) {
            k <- seq(pi/2, pi/2 - theta, length = 0.5 * length * 
                        abs(theta)/pi)
            x <- c(0, cos(k)/2, 0)
            y <- c(0, sin(k)/2, 0)
            cbind(rbind(x, y), c(NA, NA))
         }
         PIE.dat <- lapply(DAT * 2 * pi, pie.dat)
         len.pie <- unlist(lapply(PIE.dat, length))/2
         PIE.dat2 <- 0.85 * matrix(unlist(PIE.dat), ncol = 2, 
                                   byrow = TRUE)
         PIE.dat2 <- PIE.dat2 + Pos[rep(1:length(DAT), len.pie), 
         ]
         polygon(PIE.dat2, border = "black", col = col.fill)
      }
      if (method == "shade" && plotCI == "n") {
         addshade <- match.arg(addshade)
         symbols(Pos, add = TRUE, inches = FALSE, squares = rep(1, 
                                                                len.DAT), bg = col.fill, fg = addgrid.col)
         shade.dat <- function(w) {
            x <- w[1]
            y <- w[2]
            rho <- w[3]
            x1 <- x - 0.5
            x2 <- x + 0.5
            y1 <- y - 0.5
            y2 <- y + 0.5
            dat <- NA
            if ((addshade == "positive" || addshade == "all") && 
                rho > 0) {
               dat <- cbind(c(x1, x1, x), c(y, y1, y1), c(x, 
                                                          x2, x2), c(y2, y2, y))
            }
            if ((addshade == "negative" || addshade == "all") && 
                rho < 0) {
               dat <- cbind(c(x1, x1, x), c(y, y2, y2), c(x, 
                                                          x2, x2), c(y1, y1, y))
            }
            return(t(dat))
         }
         pos_corr <- rbind(cbind(Pos, DAT))
         pos_corr2 <- split(pos_corr, 1:nrow(pos_corr))
         SHADE.dat <- matrix(na.omit(unlist(lapply(pos_corr2, 
                                                   shade.dat))), byrow = TRUE, ncol = 4)
         segments(SHADE.dat[, 1], SHADE.dat[, 2], SHADE.dat[, 
                                                            3], SHADE.dat[, 4], col = shade.col, lwd = shade.lwd)
      }
      if (method == "square" && plotCI == "n") {
         symbols(Pos, add = TRUE, inches = FALSE, squares = abs(DAT)^0.5, 
                 bg = col.fill, fg = col.border)
      }
      if (method == "color" && plotCI == "n") {
         symbols(Pos, add = TRUE, inches = FALSE, squares = rep(1, 
                                                                len.DAT), bg = col.fill, fg = col.border)
      }
      symbols(Pos, add = TRUE, inches = FALSE, bg = NA, squares = rep(1, 
                                                                      len.DAT), fg = addgrid.col)
      if (plotCI != "n") {
         if (is.null(lowCI.mat) || is.null(uppCI.mat)) {
            stop("Need lowCI.mat and uppCI.mat!")
         }
         if (order != "original") {
            lowCI.mat <- lowCI.mat[ord, ord]
            uppCI.mat <- uppCI.mat[ord, ord]
         }
         pos.lowNew <- getPos.Dat(lowCI.mat)[[1]]
         lowNew <- getPos.Dat(lowCI.mat)[[2]]
         pos.uppNew <- getPos.Dat(uppCI.mat)[[1]]
         uppNew <- getPos.Dat(uppCI.mat)[[2]]
         if (!method %in% c("circle", "square")) {
            stop("method shoud be circle or square if draw confidence interval!")
         }
         k1 <- (abs(uppNew) > abs(lowNew))
         bigabs <- uppNew
         bigabs[which(!k1)] <- lowNew[!k1]
         smallabs <- lowNew
         smallabs[which(!k1)] <- uppNew[!k1]
         sig <- sign(uppNew * lowNew)
         color_bigabs <- col[ceiling((bigabs + 1) * length(col)/2)]
         color_smallabs <- col[ceiling((smallabs + 1) * length(col)/2)]
         if (plotCI == "circle") {
            symbols(pos.uppNew[, 1], pos.uppNew[, 2], add = TRUE, 
                    inches = FALSE, circles = 0.95 * abs(bigabs)^0.5/2, 
                    bg = ifelse(sig > 0, col.fill, color_bigabs), 
                    fg = ifelse(sig > 0, col.fill, color_bigabs))
            symbols(pos.lowNew[, 1], pos.lowNew[, 2], add = TRUE, 
                    inches = FALSE, circles = 0.95 * abs(smallabs)^0.5/2, 
                    bg = ifelse(sig > 0, bg, color_smallabs), fg = ifelse(sig > 
                                                                             0, col.fill, color_smallabs))
         }
         if (plotCI == "square") {
            symbols(pos.uppNew[, 1], pos.uppNew[, 2], add = TRUE, 
                    inches = FALSE, squares = abs(bigabs)^0.5, bg = ifelse(sig > 
                                                                              0, col.fill, color_bigabs), fg = ifelse(sig > 
                                                                                                                         0, col.fill, color_bigabs))
            symbols(pos.lowNew[, 1], pos.lowNew[, 2], add = TRUE, 
                    inches = FALSE, squares = abs(smallabs)^0.5, 
                    bg = ifelse(sig > 0, bg, color_smallabs), fg = ifelse(sig > 
                                                                             0, col.fill, color_smallabs))
         }
         if (plotCI == "rect") {
            rect.width <- 0.25
            rect(pos.uppNew[, 1] - rect.width, pos.uppNew[, 2] + 
                    smallabs/2, pos.uppNew[, 1] + rect.width, pos.uppNew[, 
                                                                         2] + bigabs/2, col = col.fill, border = col.fill)
            segments(pos.lowNew[, 1] - rect.width, pos.lowNew[, 
                                                              2] + DAT/2, pos.lowNew[, 1] + rect.width, pos.lowNew[, 
                                                                                                                   2] + DAT/2, col = "black", lwd = 1)
            segments(pos.uppNew[, 1] - rect.width, pos.uppNew[, 
                                                              2] + uppNew/2, pos.uppNew[, 1] + rect.width, 
                     pos.uppNew[, 2] + uppNew/2, col = "black", lwd = 1)
            segments(pos.lowNew[, 1] - rect.width, pos.lowNew[, 
                                                              2] + lowNew/2, pos.lowNew[, 1] + rect.width, 
                     pos.lowNew[, 2] + lowNew/2, col = "black", lwd = 1)
            segments(pos.lowNew[, 1] - 0.5, pos.lowNew[, 2], 
                     pos.lowNew[, 1] + 0.5, pos.lowNew[, 2], col = "grey70", 
                     lty = 3)
         }
      }
      if (!is.null(p.mat) && insig != "n") {
         if (order != "original") {
            p.mat <- p.mat[ord, ord]
         }
         pos.pNew <- getPos.Dat(p.mat)[[1]]
         pNew <- getPos.Dat(p.mat)[[2]]
         ind.p <- which(pNew > sig.level)
         p_inSig <- length(ind.p) > 0
         if (insig == "pch" && p_inSig) {
            points(pos.pNew[, 1][ind.p], pos.pNew[, 2][ind.p], 
                   pch = pch, col = pch.col, cex = pch.cex, lwd = 2)
         }
         if (insig == "p-value" && p_inSig) {
            text(pos.pNew[, 1][ind.p], pos.pNew[, 2][ind.p], 
                 round(pNew[ind.p], 2), col = pch.col)
         }
         if (insig == "blank" && p_inSig) {
            symbols(pos.pNew[, 1][ind.p], pos.pNew[, 2][ind.p], 
                    inches = FALSE, squares = rep(1, length(pos.pNew[, 
                                                                     1][ind.p])), fg = addgrid.col, bg = bg, add = TRUE)
         }
      }
      if (cl.pos != "n") {
         colRange <- assign.color(dat = cl.lim2)
         ind1 <- which(col == colRange[1])
         ind2 <- which(col == colRange[2])
         colbar <- col[ind1:ind2]
         if (is.null(cl.length)) {
            cl.length <- ifelse(length(colbar) > 20, 11, length(colbar) + 
                                   1)
         }
         labels <- seq(cl.lim[1], cl.lim[2], length = cl.length)
         if (cl.pos == "r") {
            vertical <- TRUE
            xlim <- c(m2 + 0.5 + mm * 0.02, m2 + 0.5 + mm * cl.ratio)
            ylim <- c(n1 - 0.5, n2 + 0.5)
         }
         if (cl.pos == "b") {
            vertical <- FALSE
            xlim <- c(m1 - 0.5, m2 + 0.5)
            ylim <- c(n1 - 0.5 - nn * cl.ratio, n1 - 0.5 - nn * 
                         0.02)
         }
         colorlegend(colbar = colbar, labels = round(labels, 2), 
                     offset = cl.offset, ratio.colbar = 0.3, cex = cl.cex, 
                     xlim = xlim, ylim = ylim, vertical = vertical, align = cl.align.text)
      }
      if (tl.pos != "n") {
         pos.xlabel <- cbind(m1:m2, n2 + 0.5 + laboffset)
         pos.ylabel <- cbind(m1 - 0.5, n2:n1)
         if (tl.pos == "td") {
            if (type != "upper") {
               stop("type should be \"upper\" if tl.pos is \"dt\".")
            }
            pos.ylabel <- cbind(m1:(m1 + nn) - 0.5, n2:n1)
         }
         if (tl.pos == "ld") {
            if (type != "lower") {
               stop("type should be \"lower\" if tl.pos is \"ld\".")
            }
            pos.xlabel <- cbind(m1:m2, n2:(n2 - mm) + 0.5 + laboffset)
         }
         if (tl.pos == "d") {
            pos.ylabel <- cbind(m1:(m1 + nn) - 0.5, n2:n1)
            pos.ylabel <- pos.ylabel[1:min(n, m), ]
            symbols(pos.ylabel[, 1] + 0.5, pos.ylabel[, 2], add = TRUE, 
                    bg = bg, fg = addgrid.col, inches = FALSE, squares = rep(1, 
                                                                             length(pos.ylabel[, 1])))
            text(pos.ylabel[, 1] + 0.5, pos.ylabel[, 2], newcolnames[1:min(n, 
                                                                           m)], col = tl.col, cex = tl.cex, ...)
         }
         else {
            text(pos.xlabel[, 1], pos.xlabel[, 2], newcolnames, 
                 srt = tl.srt, adj = ifelse(tl.srt == 0, c(0.5, 
                                                           0), c(0, 0)), col = tl.col, cex = tl.cex, offset = tl.offset, 
                 ...)
            text(pos.ylabel[, 1], pos.ylabel[, 2], newrownames, 
                 col = tl.col, cex = tl.cex, pos = 2, offset = tl.offset, 
                 ...)
         }
      }
      title(title, ...)
      if (!is.null(addCoef.col) && method != "number") {
         text(Pos[, 1], Pos[, 2], col = addCoef.col, labels = round((DAT - 
                                                                        int) * ifelse(addCoefasPercent, 100, 1)/zoom, number.digits), 
              cex = number.cex, font = number.font)
      }
      if (type == "full" && plotCI == "n" && !is.null(addgrid.col)) {
         rect(m1 - 0.5, n1 - 0.5, m2 + 0.5, n2 + 0.5, border = addgrid.col)
      }
      if (!is.null(addrect) && order == "hclust" && type == "full") {
         corrRect.hclust(corr, k = addrect, method = hclust.method, 
                         col = rect.col, lwd = rect.lwd)
      }
      invisible(corr)
      if (h.ratio) {
         return(h.ratio = h.ratio.val)
      }
   }
```

```{r}
integer_cols <- sapply(train, function(y){class(y) == "integer"})
integer_train <- train[, .SD, .SDcols = integer_cols]



# Correlation matrix :
train_cor <- cor(integer_train, use = "complete")

# Isolate SalePrice column and sort variables from highest cor with SalePrice
# to lowest cor
train_cor_sort <- as.matrix(sort(train_cor[,'SalePrice'], decreasing = TRUE))

# Keep cors > 0.5
high_cors <- names(which(apply(train_cor_sort, 1, function(x) abs(x)>0.5)))
train_cor <- train_cor[high_cors, high_cors]

# Corrplot2 is a modified version of corrplot that prevents labels to go out of
# the image's frame. If you're interested, it will be on my .Rmd file
corrplot2(train_cor, method = "square", type = "lower", 
         title = "Correlation plot between numeric variables",
         tl.col = "black", tl.cex = 0.6, 
         mar = c(0,0,2,0))
```

Now that the corrplot is made, we can convert the "false numeric" variables to factor :

```{r}
to_factor <- c(to_factor,"MSSubClass", "OverallQual", "OverallCond")
train[, (to_factor) := lapply(.SD, as.factor), .SDcols = to_factor]

```

### Correlation with response variable 

The three variables with the highest correlation with *SalePrice* are *OverallQual*, *GrLivArea* and *TotalBsmtSF*.

Let's have a look at their relationship :

#### Overall Quality

```{r}
ggplot(data = train, aes(x = OverallQual, y = SalePrice)) + geom_boxplot()+
   geom_text_repel(
      aes(label = ifelse((train$OverallQual == 10 & train$SalePrice < 300000), 
                         rownames(train), '')
          ))
```
#### Above grade (ground) living area square feet

```{r}
qplot(GrLivArea, SalePrice, data = train) + geom_smooth(method = "lm", se = F)+
   geom_text_repel(
      aes(label = ifelse((train$GrLivArea > 4500 | train$SalePrice > 550000), 
                         rownames(train), '')
          ))
```

#### Total square feet of basement area

```{r}
qplot(TotalBsmtSF, SalePrice, data = train) + geom_smooth(method = "lm", se = F)+
   geom_text_repel(
      aes(label = ifelse((train$TotalBsmtSF > 3000 | train$SalePrice > 550000), 
                         rownames(train), '')
          ))
```


#### First inferences

From only these 3 most important numeric variables, we can see that there are outliers that have either abnormally high prices or abnormally low prices. The row number of these outliers are displayed on the plots.

Therefore, we might want to remove them from the training set and, overall, build models that are robust to outliers, which means that random forests should not be adapted to our case study.

### Correlation between predictors

We find that there are some variables that have strong correlations with each other :

- GarageCars and GarageArea
- 1stFlrSF (First Floor square feet) and TotalBsmtSF (Total square feet of basement area)
- TotRmsAbvGrd (Total rooms above grade) and GrLivArea (Above grade living area square feet)

Below are their correlation coefficients :

```{r}
cor(train$GarageCars, train$GarageArea)
cor(train$X1stFlrSF, train$TotalBsmtSF)
cor(train$TotRmsAbvGrd, train$GrLivArea)
```

Moreover, GarageCars, GrLivArea and TotalBsmtSF are heavily correlated to other variables. We would therefore tend to remove them from the dataset. However, they are also heavily related to the response variable ; we might risk losing accuracy if we remove them. 

Therefore, we will need to do some feature engineering at some point to combine redundant variables that are highly correlated to each other.


# Back to NAs : imputing values

We previously took a look at our dataset's missing values and focused on those that meant "none" rather that actual NAs.

Now, we will work on the latter and see how we can input values to them :

```{r}
temp_na <- as.numeric(train[, lapply(.SD, function(y){sum(is.na(y))})])
names(temp_na) <- names(train)
temp_na <- temp_na[temp_na > 0]
print(temp_na)
```

Except for *LotFrontage*, most variables have a low number of NAs, which are located in the following observations :

```{r}
which(is.na(train$MasVnrType))
which(is.na(train$MasVnrArea))
which(is.na(train$Electrical))
```
By removing 5 rows, we can make it so that we only have to focus on the *LotFrontage* variable, so that's what we'll do :

```{r}
row_remove <- unique(c(which(is.na(train$MasVnrType)), 
                       which(is.na(train$MasVnrArea)),
                       which(is.na(train$Electrical))))
train <- train[-row_remove, ]
```

## LotFrontage 

The *LotFrontage* variable refers to the linear feet of street connected to a property. 

For each missing value, we'll input the mean of the *LotFrontage* value for the associated neighborhood. This means we assume that, for every neighborhood, the *LotFrontage* values are rather condensed around a mean value. 

Actually, this can be confirmed by looking at this plot :

```{r}
ggplot(data = train, aes(x = Neighborhood, y = LotFrontage)) + geom_boxplot() + 
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

We'll create a data table with contains, for each neighborhood, the mean of its *LotFrontage*, and set the *Neighborhood* values as keys to call the associated mean value when needed :

```{r}
neig_lf_mean <- train %>% group_by(Neighborhood) %>% 
   summarize(meanLF = mean(LotFrontage, na.rm = T)) %>%
   mutate(meanLF = round(meanLF))

neig_lf_mean <- data.table(neig_lf_mean)

setkey(neig_lf_mean, Neighborhood)
```

Now, for each missing value of *LotFrontage*, we extract the associated *Neighborhood* value, call its *LotFrontage* mean thanks to our new data table, and input it to our train set :

```{r}
for (i in 1:nrow(train)){
   if (is.na(train$LotFrontage[i])){
      ngbr <- train$Neighborhood[i]
      lf_mean <- neig_lf_mean[.(ngbr)]$meanLF # calling the mean with the key
      train$LotFrontage[i] <- lf_mean
   }
}
```

## Checking NAs 

Now, we should have no remaining missing value in our set :

```{r}
temp_na <- as.numeric(train[, lapply(.SD, function(y){sum(is.na(y))})])
names(temp_na) <- names(train)
temp_na <- temp_na[temp_na > 0]
print(temp_na)
```

# Variable importance

Correlation plots are a quick and efficient way to check the most important variables but can only be applied to numeric variables. To get a true indication of which variables are the most important, we have to take into account the categorical variables, and the easiest way to do that is to use the *importance* variable of a random forest model fit. 

We previously said that random forest models would be a poor fit for this study, but that is only because it is not the best one to use when we have outliers. other than that, we can still trust the results such a model would give us concerning variable importance.

Now we have to keep in mind here that we only want to have an indication of variable importance, so we will not do any pre-processing like we would do during our actual model building :

```{r}
set.seed(1234)
modfit_rf <- randomForest(SalePrice ~ ., data = train, 
                          ntree = 200, 
                          importance = T)
imp_rf <- importance(modfit_rf)
imp_rf <- data.table(variables = row.names(imp_rf), inc_mse = imp_rf[, 1])
imp_rf <- arrange(imp_rf, desc(inc_mse))

ggplot(data = filter(imp_rf, inc_mse >= 5), 
       aes(x = reorder(variables, -inc_mse), y = inc_mse, fill = inc_mse)) + 
   geom_bar(stat = "identity") +
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
         legend.position = "none")+
   labs(x = "Variables", 
        y = "% increase in MSE if variable is shuffled")
```
Among the most important variables, we have a mix of categorical and numeric variables, which means that we have to use a model that properly uses large amounts of categorical data.



# Feature engineering

## Area Variables

Our dataset contains several variables that deal with areas which have some redundancies :

```{r}
head(select(train, X1stFlrSF, X2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF))
```
*GrLivArea* seems to be the sum of *X1stFlrSF*, *X2ndFlrSF*, and *LowQualFinSF*. Similarly, *TotalBsmtSF* seems to be the sum of *BsmtFinSF1*, *BsmtFinSF2*, and *BsmtUnfSF*. Let's verify it :

```{r}
with(train, cor((X1stFlrSF + X2ndFlrSF + LowQualFinSF), GrLivArea))
with(train, cor((BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF), TotalBsmtSF))
```
The correlation is exactly 1, which means our assumption was correct. What we'll do here is that we will combine *GrLivArea* and *TotalBsmtSF* into a single *TotalSF* variable and delete the rest :

```{r}
train[, TotalSF := GrLivArea + TotalBsmtSF]
train <- select(train, -c("X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", 
                "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF"))
```

## Bathroom variables 

Our dataset also contains several bathroom variables that will be combined into and replaced by a single variable. Variables labeled as "HalfBath" will be counted for half :

```{r}
train[, TotalBath := FullBath + (0.5 * HalfBath) + BsmtFullBath + (0.5 * BsmtHalfBath)]
train <- select(train, -c("FullBath", "HalfBath", "BsmtFullBath",  "BsmtHalfBath"))
```


## House age and remodeling

The dataset provides us with two variables *YearBuilt* and *YearSold*. Intuitively, one would say that a house's age has a significant impact on its price. Therefore we will create a new variable which will be the age of the house when sold.


In addition, we will replace the *YearRemodAdd* by a simpler variable, *RemodAdd* that will be 1 if the house has been remodeled and 0 if not

```{r}
train[, HouseAge := YrSold - YearBuilt]
train[, RemodAdd := ifelse(YearBuilt == YearRemodAdd, 0, 1)]
train[, RemodAdd := factor(RemodAdd)]
train <- select(train, -c("YearBuilt", "YrSold", "YearRemodAdd"))
```

### Quick focus on house age

There is an interesting - but not so surprising - pattern that appears when plotting the relationship between *HouseAge* and *SalePrice* :

```{r}
g1 <- ggplot(data = train, aes(x = HouseAge, y = SalePrice)) + geom_point() +
   ggtitle("Sale Price by House Age") + labs(x = "House Age", y = "Sale Price")
g2 <- ggplot(data = filter(train, HouseAge > 1), aes(x = HouseAge, y = SalePrice))+ 
   geom_point() + ggtitle("Sale Price for House Age > 1 year") + 
   labs(x = "House Age", y = "Sale Price")
grid.arrange(g1, g2)
```

The first plot takes all the houses into account whereas the second only take houses aged 2 years or more. The linear relationship is more visible in the second plot and we can see that new houses (1 year or less) follow a different pattern.

That means that in addition to *HouseAge*, it might be relevant to add a *NewHouse* variable that is 1 if the house is new and 0 if not :

```{r}
train[, NewHouse := ifelse(HouseAge <= 1, 1, 0)]
train[, NewHouse := factor(NewHouse)]
```

Let's see if we notice a significant difference between these two categories :

```{r}
ggplot(data = train, aes(x = NewHouse, y = SalePrice, fill = NewHouse)) + 
   geom_boxplot() + theme(legend.position = "none")+
   scale_y_continuous(labels = comma)
```

New houses seem to generally have a higher sale price.

## Neighborhood

Neighborhood is one of the most important variable. However, it is not only categorical, but also has a lot of levels. To make it easier for our model to take neighborhood into account, we want to split it into categories such as poor, medium and expensive. 

Let's see if there is a clear separation that can be used to classify them. To do that, we will plot the mean sale price for every neighborhood :


```{r}
neig_sp_mean <- train %>% group_by(Neighborhood) %>% summarise(mn = mean(SalePrice))

spmean <- mean(train$SalePrice)

ggplot(data = neig_sp_mean, aes(x = reorder(Neighborhood, mn), y = mn, fill = mn)) + 
   geom_bar(stat = "identity")+
   labs(x = "Neighborhood", y = "Mean Sale Price")+
   scale_y_continuous(breaks= seq(0, 800000, by=50000))+
   ggtitle("Mean Sale Price by Neighborhood")+
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
         legend.position = "none")+
   geom_hline(yintercept = spmean, color = "red")+


   geom_text(aes(0, spmean, label = "Dataset mean sale price", vjust = -1, hjust = -0.1))
   


```
We can actually see 4 categories :

- 3 neighborhoods appear to have mean sale prices far above the others
- Similarly, 3 neighborhood seem to have lower mean sale prices
- As for the other neighborhood, there seem to be a separation between Mitchel and SawyerW, around the dataset mean sale price.

Therefore, we will apply the following classification for neighborhoods :

```{r}
ggplot(data = neig_sp_mean, aes(x = reorder(Neighborhood, mn), y = mn, fill = mn)) + 
   geom_bar(stat = "identity")+
   labs(x = "Neighborhood", y = "Mean Sale Price")+
   scale_y_continuous(breaks= seq(0, 800000, by=50000))+
   ggtitle("Neighborhood Categories")+
   theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
         legend.position = "none")+
   geom_hline(yintercept = spmean, color = "red")+
   geom_vline(xintercept = 3.5, color = "green", size = 1.2)+
   geom_vline(xintercept = 12.5, color = "green", size = 1.2)+
   geom_vline(xintercept = 22.5, color = "green", size = 1.2)+

   geom_text(aes(1.25, 320000, label = "Poor", vjust = -1, hjust = -0.1))+
   geom_text(aes(6.5, 320000, label = "Med-low", vjust = -1, hjust = -0.1))+   
   geom_text(aes(16, 320000, label = "Med-high", vjust = -1, hjust = -0.1))+   
   geom_text(aes(23.25, 320000, label = "Rich", vjust = -1, hjust = -0.1))

```

```{r}
neig_classify <- function(x){
   if (x %in% c("MeadowV", "IDOTRR", "BrDale")){
      return(0)
   }else if (x %in% c("BrkSide", "Edwards", "OldTown", "Sawyer", "Blueste", 
                      "NPkVill", "SWISU", "NAmes", "Mitchel")){
      return(1)
   }else if (x %in% c("SawyerW", "NWAmes", "Gilbert", "Blmngtn", "CollgCr",
                      "Crawfor", "ClearCr", "Somerst", "Veenker", "Timber")){
      return(2)
   }else if (x %in% c("StoneBr", "NridgHt", "NoRidge")){
      return(3)
   }else{
      return(NA)
   }
}

train[, NeighClass := neig_classify(Neighborhood), by = seq_len(nrow(train))]
train[, NeighClass := factor(NeighClass)]
train <- select(train, -Neighborhood)
```


# Pre-processing

We're almost ready to build our model, but before we do so we need to preprocess our data by :

- Removing reduncancies (highly correlated variables)
- Removing outliers
- Updating factor levels
- Normalize numeric variables
- Encoding categorical variables
- Normalize response variable

## Back to correlations

Our dataset has now changed quite a bit : most redundancies have been removed by combining variables into one. Here, we'll have another quick look at correlations to see if there are some highly correlated variables left :


```{r}
integer_cols2 <- sapply(train, function(y){(class(y) == "integer") | (class(y) == "numeric")})
integer_train2 <- train[, .SD, .SDcols = integer_cols2]



# Correlation matrix :
train_cor2 <- cor(integer_train2, use = "complete")


# Isolate SalePrice column and sort variables from highest cor with SalePrice
# to lowest cor
train_cor_sort2 <- row.names(as.matrix(sort(train_cor2[,'SalePrice'], decreasing = TRUE)))

# Keep cors > 0.5
#high_cors2 <- names(which(apply(train_cor_sort2, 1, function(x) abs(x)>0)))
train_cor2 <- train_cor2[train_cor_sort2, train_cor_sort2]

# Corrplot2 is a modified version of corrplot that prevents labels to go out of
# the image's frame. If you're interested, it will be on my .Rmd file
corrplot2(train_cor2, method = "square", type = "lower", 
         title = "Correlation plot between numeric variables",
         tl.col = "black", tl.cex = 0.6, 
         mar = c(0,0,2,0))
```

These variables seem to be highly correlated :

- *GarageArea* and *GarageCars*
- *TotRmsAbvGrd* and *TotalSF*
- *TotRmsAbvGrd* and *BedroomAbvGr*

```{r}
with(train, cor(GarageArea, GarageCars))
with(train, cor(TotRmsAbvGrd, TotalSF))
with(train, cor(TotRmsAbvGrd, BedroomAbvGr))
```
We'll keep *GarageCars*, *TotalSF* that have a higher correlation with *SalePrice* and remove *GarageArea* and *TotRmsAbvGrd*

In addition, the *MoSold* variable has no correlation with *SalePrice* nor with any variable. This confirms an intuitive assumption that, globally, the month of the sale has little impact on the sale price. We will also remove this variable

```{r}
train <- select(train, -c("GarageArea", "TotRmsAbvGrd", 'MoSold'))
```

## Outliers

We saw during the exploratory data analysis that our 364th overvation was an outlier. To improve our model's accuracy, we will remove it from the training set

```{r}
train <- train[-364, ]
```

## Levels

At the very beginning of our study, we split our original training set into a new train set and a sub-test to do some model verification.

By doing so, we created a minor issue that needs fixing : factor levels discrepancies between the datasets. Some factor variables have levels that almost never appear. By splitting the original data into two, one level might never appear in the training set but will appear in our subtest or testing sets. That will confuse our model during prediction.

Even if a level never appears in the training set, our model needs to know that it still exists. To do that, the following code will :

- re-open the original training set
- Pre-process it the same way we did so far to the train set
- extract all levels for every factor variable
- apply it to our training set variables

```{r}
train_original <- read.csv(trainfile)
train_original <- data.table(train_original)

# Factorize variables :
for (j in names(temp_na2)){
   set(train_original,which(is.na(train_original[[j]])),j,"None")
}
train_original[, (to_factor) := lapply(.SD, as.factor), .SDcols = to_factor]

# Feature engineering + Removing variables 
train_original <- select(train_original, -c("Id", "GarageYrBlt"))
train_original[, TotalSF := GrLivArea + TotalBsmtSF]
train_original <- select(train_original, -c("X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", 
                "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF"))
train_original[, TotalBath := FullBath + (0.5 * HalfBath) + BsmtFullBath + (0.5 * BsmtHalfBath)]
train_original <- select(train_original, -c("FullBath", "HalfBath", "BsmtFullBath",  "BsmtHalfBath"))
train_original[, HouseAge := YrSold - YearBuilt]
train_original[, RemodAdd := ifelse(YearBuilt == YearRemodAdd, 0, 1)]
train_original[, RemodAdd := factor(RemodAdd)]
train_original <- select(train_original, -c("YearBuilt", "YrSold", "YearRemodAdd"))
train_original[, NewHouse := ifelse(HouseAge <= 1, 1, 0)]
train_original[, NewHouse := factor(NewHouse)]
train_original[, NeighClass := neig_classify(Neighborhood), by = seq_len(nrow(train_original))]
train_original[, NeighClass := factor(NeighClass)]
train_original <- select(train_original, -Neighborhood)
train_original <- select(train_original, -c("GarageArea", "TotRmsAbvGrd", "MoSold"))

# Extracting levels
level_list <- list()
for (j in names(train_original)){
   if (class(train_original[[j]]) == "factor"){
      level_list <- c(level_list, list(levels(train_original[[j]])))
   }else{
      level_list <- c(level_list, list(NULL))
   }
}

# Apply to training set :

for (j in 1:dim(train)[2]){
   if (class(train[[names(train)[j]]]) == "factor"){
      levels(train[[names(train)[j]]]) <- level_list[[j]]
   }
}

```

## Normalize numeric variables

```{r}
numcols <- sapply(train, function(y){(class(y) == "integer") | (class(y) == "numeric")})
num_train <- train[, .SD, .SDcols = numcols]
num_train <- select(num_train, -SalePrice)
```


# Model Building

For now, we will build a model using the random forest algorithm, and see how well it performs.
If it's lacking, we'll try using a combination of models.

```{r, eval=F}
modfit_rf <- randomForest(SalePrice ~ ., data = proc_train)
modfit_boost <- gbm(SalePrice ~ ., data = proc_train)

```

```{r, echo=F}
load("./data/modfit_rf.RData")
load("./data/modfit_boost.RData")
```

We'll preprocess the sub-test set in the same way we did the train set :


```{r}
for (j in names(temp_na2)){
   set(subtest,which(is.na(subtest[[j]])),j,"None")
}

subtest[, (to_factor) := lapply(.SD, as.factor), .SDcols = to_factor]

subtest <- subtest[, -c(1,60)]
subtest <- subtest[, -c(38, 46, 60)]

proc_subtest <- data.table(predict(preObj, subtest[, -76]))

for (j in 1:dim(proc_subtest)[2]){
   if (class(proc_subtest[[names(proc_subtest)[j]]]) == "factor"){
      levels(proc_subtest[[names(proc_subtest)[j]]]) <- level_list[[j]]
   }
}

```

And now we predict :

```{r}
rf_predict <- predict(modfit_rf, proc_subtest)
boost_predict <- predict(modfit_boost, proc_subtest)
```

```{r}
comparison <- data.table(results = subtest$SalePrice, predicted_rf = rf_predict,
                         predicted_boost = boost_predict)
RMSE(comparison$results, comparison$predicted_rf, na.rm = T)
RMSE(comparison$results, comparison$predicted_boost, na.rm = T)
mean(comparison$results)
qplot(results, predicted_rf, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
qplot(results, predicted_boost, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")

```

```{r}
numer_train <- proc_train %>% select(which(sapply(.,is.numeric)))

numer_train <- select(numer_train, -SalePrice)
svd1 <- svd(scale(numer_train))
qplot(1:dim(numer_train)[2], svd1$d^2 / sum(svd1$d^2)) + 
   labs(x = "columns", y = "Proportion of variance explained") +
   ggtitle("Proportion of variance explained per variable")
```

```{r}
preproc <- preProcess(numer_train, method = "pca", pcaComp = 20) 

numer_train <- data.table(predict(preproc, numer_train))
numer_train[ , Neighborhood := proc_train$Neighborhood] 
numer_train[ , SalePrice := proc_train$SalePrice]
```

```{r, eval=F}
modfit_glm <- train(SalePrice ~ ., method = "glm", data = numer_train)
```

```{r, echo=F}
load(file = "./data/modfit_glm.RData")
```

Preprocessing the subtest set :

```{r}
numer_subtest <- proc_subtest %>% select(which(sapply(.,is.numeric)))
numer_subtest <- data.table(predict(preproc, numer_subtest))
numer_subtest[ , Neighborhood := proc_subtest$Neighborhood] 
```

Prediction :

```{r}
predicted_glm <- predict(modfit_glm, numer_subtest)
```

```{r}
comparison[, predicted_glm := predicted_glm]
RMSE(comparison$results, comparison$predicted_glm, na.rm = T)
qplot(results, predicted_glm, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
```

```{r}
exp_numtrain <- filter(numer_train, SalePrice > 300000)
```

```{r, eval=F}
modfit_expglm <- train(SalePrice ~ ., method = "glm", data = exp_numtrain)
```

```{r, echo=F}
load(file = "./data/modfit_expglm.RData")
```

Filter for proc_train saleprice > 300k

```{r}
exp_proctrain <- filter(proc_train, SalePrice > 300000)
```

New modfits for saleprice > 300k

```{r, eval=F}
modfit_exprf <- randomForest(SalePrice ~ ., data = exp_proctrain)
modfit_expboost <- gbm(SalePrice ~ ., data = exp_proctrain)
```
```{r,echo=F}
load(file = "./data/modfit_exprf.RData")
load(file = "./data/modfit_expboost.RData")
```

```{r}
predicted_exprf <- predict(modfit_exprf, proc_subtest)
predicted_expboost <- predict(modfit_expboost, proc_subtest)
predicted_expglm <- predict(modfit_expglm, numer_subtest)

comparison[, predicted_exprf := predicted_exprf]
comparison[, predicted_expboost := predicted_expboost]
comparison[, predicted_expglm := predicted_expglm]
RMSE(comparison$results, comparison$predicted_exprf, na.rm = T)
RMSE(comparison$results, comparison$predicted_expboost, na.rm = T)
RMSE(comparison$results, comparison$predicted_expglm, na.rm = T)
qplot(results, predicted_exprf, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
qplot(results, predicted_expboost, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
qplot(results, predicted_expglm, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
```

combined :

```{r}

pred <- numeric()
for (i in 1:dim(comparison)[1]){
   if (comparison$results[i] <= 300000){
      pred <- c(pred, sum(comparison$predicted_rf[i], comparison$predicted_boost[i], 
                    comparison$predicted_glm[i], na.rm = T)/
                   sum(3-sum(is.na(c(comparison$predicted_rf[i], 
                                   comparison$predicted_boost[i], 
                                   comparison$predicted_glm[i])))))
                  # Kind of messy way to creat a mean, but mean() with an NA
                  # returns a NaN even with na.rm = T
   }else{
      pred <- c(pred, sum(comparison$predicted_exprf[i], comparison$predicted_expboost[i], 
                    comparison$predicted_expglm[i], na.rm = T)/
                   sum(3-sum(is.na(c(comparison$predicted_exprf[i], 
                                   comparison$predicted_expboost[i], 
                                   comparison$predicted_expglm[i])))))
   }
}

comparison[, prediction := pred]

RMSE(comparison$results, comparison$prediction, na.rm = T)
qplot(results, prediction, data = comparison) + geom_abline(slope = 1, intercept = 0, colour = "red")
```
Without outlier :

```{r}
temp <- copy(comparison)
temp[, delta := abs(results - prediction)]

rem_row <- which(temp$delta >= quantile(temp$delta)[5])
temp <- temp[-rem_row, ]
RMSE(temp$results, temp$prediction, na.rm = T)
qplot(results, prediction, data = temp) + geom_abline(slope = 1, intercept = 0, colour = "red")
```